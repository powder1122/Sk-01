{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1949ff6",
   "metadata": {},
   "source": [
    "# 6월 24일\n",
    "tensor_proj repository create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0cb962",
   "metadata": {},
   "source": [
    "딥러닝, TensorFlow, Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd7b6a",
   "metadata": {},
   "source": [
    "# 딥러닝\n",
    "- 딥러닝의 정의와 활용 분야\n",
    "\n",
    "- 머신러닝과 딥러닝의 차이\n",
    "\n",
    "- 딥러닝 모델의 기본 구성 요소(입력층, 은닉층, 출력층)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633717a",
   "metadata": {},
   "source": [
    "## 딥러닝의 정의와 활용 분야\n",
    "\n",
    "### 딥러닝\n",
    "- 사람의 뇌가 정보를 처리하고 학습하는 방식을 모방한 **인공 신경망을 기반으로 한 머신러닝의 하위 분야**\n",
    "\n",
    "- 딥러닝의 핵심은 데이터를 바탕으로 복잡한 패턴을 학습하고 문제를 해결하는 것\n",
    "\n",
    "- 계층적인 데이터 학습\n",
    "    - 딥러닝은 **여러 층(레이어)**으로 구성된 신경망을 사용\n",
    "    - 각 신경망 층은 데이터에서 점점 더 높은 수준의 추상적인 특징을 학습할 수 있음\n",
    "        - 예: 이미지 데이터 --> 첫 번쨰 층: 엣지(선)\n",
    "        -                     두 번째 층: 간단한 패턴(모양)\n",
    "        -                     세 번째 층: 복잡한 패턴(얼굴, 사물 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbcb85",
   "metadata": {},
   "source": [
    "- 딥러닝의 주요 특징\n",
    "    - **비정형 데이터 처리에 강함**\n",
    "        - 딥러닝은 텍스트, 이미지, 음성 같은 구조화되지 않은 데이터를 처리하는 데 매우 강력함\n",
    "        - 이러한 데이터를 처리할 때 사람이 별도로 특징을 추출할 필요 없이 모델이 데이터를 통해 직접 학습\n",
    "\n",
    "    - **대규모 데이터**와 연산 요구\n",
    "        - 딥러닝은 대량의 데이터와 강력한 계산 자원(GPU/TPU)가 필요\n",
    "        - 데이터를 많이 사용할수록 더 정교한 모델을 학습할 수 있음\n",
    "\n",
    "    - 자동화된 학습\n",
    "        - 딥러닝은 데이터를 기반으로 **스스로 학습 규칙을 생성**\n",
    "        - 이 규칙을 통해 새로운 데이터에 대해 예측이나 분류를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb3769",
   "metadata": {},
   "source": [
    "- 딥러닝의 활용 분야\n",
    "    - 컴퓨터 **비전**: 이미지 인식, 객체 탐지, 자율 주행(영상 관련 수요 증가)\n",
    "    - 자연어 처리(NLP): 번역, 텍스트 요약, 챗봇(LLM등을 사용한 모델. GPT-->텍스트 생성, 생성형AI)\n",
    "    - 음성 처리: 음성 인식, 음성 합성\n",
    "    - 의료: 질병 진단, 의료 영상 분석 (개인정보 보호법)\n",
    "    - 추천 시스템: 사용자 맞춤 추천\n",
    "    - 금융: 사기 탐지, 금융 시장 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eda694",
   "metadata": {},
   "source": [
    "## 머신러닝과 딥러닝의 차이\n",
    "### 머신러닝과 딥러닝\n",
    "- 머신러닝\n",
    "    - 컴퓨터가 명시적인 프로그래밍 없이 데이터를 학습하여 예측이나 분류를 수행할 수 있게 하는 기술\n",
    "    - 데이터에서 **특징(Feature)**을 추출하고, 이 특징을 바탕으로 패턴을 학습하여 결과를 예측\n",
    "    - 머신러닝 알고리즘은 종종 사람이 수작업으로 **데이터를 전처리**하고 중요한 **특징**을 선정해야 함\n",
    "\n",
    "- 딥러닝\n",
    "    - 머신러닝의 하위 분야로, 인공 신경망을 기반으로 한 기술\n",
    "    - 데이터를 학습하는 과정에서 **특징을 자동으로 추출**\n",
    "    - 계층적인 학습(여러 레이어)을 통해 복잡한 패턴을 이해하고 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7253587",
   "metadata": {},
   "source": [
    "### 특징 엔지니어링\n",
    "- 머신러닝\n",
    "    - 데이터를 학습하기 전에 주요 특징을 선정하고, 이를 모델에 제공해야 함\n",
    "        - 예: 이미지에서 색상, 모서리, 텍스처 같은 특징을 사람이 추출\n",
    "    - 알고리즘은 제공된 특징을 기반으로 학습 --> 사람이 모두 하기엔 무리가 있다!\n",
    "\n",
    "- 딥러닝\n",
    "    - 자동 특징 학습이 가능\n",
    "    - 모델이 원시 데이터(raw data)를 입력 받아 데이터에서 특징을 자동으로 추출하고 학습\n",
    "        - 예: 이미지 데이터를 입력하면 모델이 자동으로 엣지, 패턴, 사물을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56c9d1",
   "metadata": {},
   "source": [
    "### 데이터 요구량\n",
    "- 머신러닝\n",
    "    - 상대적으로 적은 양의 데이터로도 학습이 가능\n",
    "    - 작은 데이터셋에서도 좋은 성능을 낼 수 있도록 설계된 알고리즘이 많음\n",
    "\n",
    "- 딥러닝\n",
    "    - 매우 큰 데이터셋이 필요\n",
    "    - 딥러닝 모델은 계층이 깊고 복잡하기 때문에, 충분히 많은 데이터를 사용해야 높은 성능을 보장\n",
    "        - 예: 자율주행 차량의 딥러닝 모델은 수백만 개의 이미지와 동영상을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee44c2",
   "metadata": {},
   "source": [
    "### 연산 자원\n",
    "- 머신러닝\n",
    "    - 일반적인 CPU에서도 작동 가능하며, 연산 자원 요구가 상대적으로 적음\n",
    "    - 모델이 단순하고 학습 과정이 비교적 빠름\n",
    "\n",
    "- 딥러닝\n",
    "    - GPU나 TPU같은 강력한 연산 자원이 필요\n",
    "    - 모델의 계층이 많아 학습 과정이 시간이 많이 걸리고, 계산 복잡도가 높음\n",
    "\n",
    "### 강인공지능과 약인공지능\n",
    "- 인공지능의 분류\n",
    "    - 약인공지능\n",
    "        - 한 분야를 집중해 인공지능을 실현\n",
    "        - 음성인식, 얼굴인식 등\n",
    "    \n",
    "    - 강 인공지능\n",
    "        - 인간이 하는 행동 수준 또는 그 이상의 행동\n",
    "        - 예: 터미네이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fb5a0",
   "metadata": {},
   "source": [
    "## 인공 신경망\n",
    "### 인공 신경망\n",
    "- 인공 신경망(Artificial Neural Network, ANN)\n",
    "    - 컴퓨터가 사람의 뇌를 본떠서 만든 알고리즘\n",
    "    - 뇌에는 뉴련이라는 작은 세포들이 서로 연결되어 정보를 주고 받으며 생각하고 결정\n",
    "    - 인공 신경망도 이와 비슷하게 여러 노드가 연결되어 정보를 처리\n",
    "\n",
    "- 뉴런의 구조\n",
    "    - 신경세포체\n",
    "        - 뉴런의 중심 부분으로, 핵과 다양한 세포 소기관이 포함\n",
    "        - 세포의 생명 유지와 대사 활동을 담당하며, 단백질 합성 등 중요한 기능을 수행\n",
    "\n",
    "    - 가지돌기\n",
    "        - 신경세포체에서 나뭇가지처럼 뻗어 나온 짧은 돌기들로, **다른 뉴런으로부터 신호를 받아들이는 역할**(특징들을 찾아가는 과정)\n",
    "        - 여러 개의 가지돌기를 통해 동시에 다양한 신호를 수신\n",
    "    \n",
    "    - 축삭돌기\n",
    "        - 신경세포체에서 길게 뻗어 나온 하나의 돌기로, 전기 신호를 먼 거리까지 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9b752",
   "metadata": {},
   "source": [
    "### 퍼셉트론\n",
    "- 인공 신경망의 가장 기본적인 구성 요소\n",
    "- 여러 입력을 받아 하나의 출력을 생성하는 단순한 모델\n",
    "- 1957년 프랭크 로젠블랫에 의해 개발\n",
    "\n",
    "- 퍼셉트론의 구조\n",
    "    - 입력\n",
    "        - 퍼셉트론은 여러 개의 입력 값을 받아들임(예: x1, x2, x3와 같은 데이터가 들어옴)\n",
    "        - 입력은 우리가 처리하고 싶은 정보(예: 이미지의 픽셀 값 등)를 의미\n",
    "    \n",
    "    - 가중치\n",
    "        - 각 입력에는 가중치라는 숫자가 곱해짐\n",
    "        - 가중치는 입력 값이 얼마나 중요한지를 나타내는 역할을 함\n",
    "\n",
    "    - 편향(Bias)\n",
    "        - 편향은 계산에 더해지는 추가 값 --> 퍼셉트론이 더 유연하게 작동하도록 함\n",
    "        - 예를 들어, 출력 값이 항상 0이 되는 것을 막아주는 역할.(y절편)\n",
    "\n",
    "    - 합산기\n",
    "        - 각 입력에 가중치를 곱한 후 모두 더하고 편향 값을 추가. S값은 퍼셉트론이 다음 단계로 넘어가도록 만드는 기준\n",
    "\n",
    "    - 활성화 함수\n",
    "        - 합산된 값 S가 특정 임계값을 넘는지 확인\n",
    "        - 이 과정은 스위치를 켜고 끄는 것처럼 동작\n",
    "\n",
    "    - 출력\n",
    "        - 최종적으로 0 또는 1의 값을 출력\n",
    "        - 퍼셉트론이 '이 데이터는 어떤 부류에 속한다' 라고 판단한 결과\n",
    "\n",
    "- 퍼셉트론의 동작 원리\n",
    "    - 입력 신호 수집: 입력층에서 여러 개의 입력 신호를 받음\n",
    "    - 가중합 계산: 각 입력 신호에 해당 가중치를 곱한 후, 모든 값을 합산하고 편향을 더하여 총합S를 계산\n",
    "    - 활성화 함수 적용: 계산된 총합 S를 활성화 함수에 입력하여 출력값을 결정\n",
    "    - 결과 출력: 총합이 임계값을 넘으면 1, 그렇지 않으면 0을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a42d8a",
   "metadata": {},
   "source": [
    "- 퍼셉트론의 학습\n",
    "    - 퍼셉트론은 주어진 데이터에 대해 **가중치와 편향을 조정**하여 원하는 출력이 나오도록 학습\n",
    "\n",
    "    - 학습 과정\n",
    "        1. 초기화: 가중치와 편향을 임의의 값으로 설정\n",
    "        2. 예측 및 오차 계산: 입력 데이터를 통해 예측 값을 계산하고, 실제 값과의 차이(오차)를 구함\n",
    "        3. 가중치 및 편향 업데이트: 오차를 기반으로 가중치와 편향을 조정하여 모델이 점차 정확한 예측을 할 수 있도록 함\n",
    "        4. 반복: 오차가 최소화될 때까지 도는 정해진 횟수만큼 위 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44d598",
   "metadata": {},
   "source": [
    "### 퍼셉트론의 한계\n",
    "- 퍼셉트론은 간단한 선형 분리 문제(데이터를 직선으로 나눌 수 있는 문제)는 잘 해결\n",
    "\n",
    "- XOR문제처럼 복잡한 문제를 해결하지 못함\n",
    "- 이를 극복하기 위해 다층 퍼셉트론(MLP)과 같은 복잡한 구조가 개발 -> 여러 개의 은닉층을 통해 복잡한 패턴을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21dfaf6",
   "metadata": {},
   "source": [
    "### 인공 신경망의 구성 요소\n",
    "1. 뉴런\n",
    "    - 인공 신경망의 기본 단위로, 생물학적 뉴런을 모방한 구조\n",
    "    - 각 뉴런은 입력 신호를 받아 계산을 수행하고, 결과를 다음 층으로 전달\n",
    "    - 뉴런의 동작\n",
    "        - 입력 신호와 가중치를 곱한 값을 합산하고, 편향(bias)을 더함\n",
    "        - 결과를 활성화 함수에 통과시켜 최종 출력값을 생성\n",
    "\n",
    "2. 가중치\n",
    "    - 뉴런 간 연결의 강도를 나타내는 값\n",
    "    - 각 입력 데이터는 고유한 가중치를 가지며, 학습 과정에서 이 값이 조정됨\n",
    "    - 가중치는 신경망이 학습하여 패턴을 인식하는 데 중요한 역할을 함\n",
    "\n",
    "3. 편향(bias)\n",
    "    - 가중치와 입력값의 합에 더해지는 상수 값으로, 뉴런의 출력값을 조정하는 역할\n",
    "    - 모델이 특정한 방향으로 데이터를 더 잘 학습하도록 지원\n",
    "\n",
    "4. 층(Layer)\n",
    "    - 신경망은 여러 층으로 구성되며, 각각의 층은 뉴런의 집합\n",
    "        - 입력층: 데이터를 입력받는 층\n",
    "        - 은닉층: 입력 데이터를 처리하고 학습하는 층\n",
    "        - 출력층: 최종 결과를 출력하는 층\n",
    "\n",
    "5. 활성화 함수\n",
    "    - 뉴런의 출력을 비선형으로 변환하여 복잡한 패턴을 학습할 수 있게 함\n",
    "    - 대표적인 활성화 함수\n",
    "        - ReLU\n",
    "        - Sigmoid\n",
    "        - Tanh\n",
    "        - Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086437a8",
   "metadata": {},
   "source": [
    "### 인공 신경망의 구조\n",
    "- 신경망은 입력층, 은닉층, 출력층의 계층적 구조\n",
    "\n",
    "1. 입력층\n",
    "    - 데이터를 입력받는 층으로, 입력 데이터의 각 특징은 하나의 뉴런에 대응\n",
    "        - 예: 이미지 데이터 28*28픽셀 --> 입력층 뉴런 수: 784개\n",
    "\n",
    "2. 은닉층\n",
    "    - 신경망의 핵심 부분으로, 데이터의 패턴을 학습\n",
    "    - 각 은닉층의 뉴런은 이전 층의 모든 뉴런과 연결되며, 이를 완전 연결이라고 함\n",
    "    - 은닉층의 개수와 뉴런 수는 문제의 복잡도에 따라 결정\n",
    "    - 다층 구조의 은닉층을 가진 신경망을 심층 신경망이라고 함\n",
    "\n",
    "3. 출력층\n",
    "    - 최종 결과를 제공하는 층으로, 문제 유형에 따라 뉴런 수와 활성화 함수가 달라짐\n",
    "        - 회귀 문제: 출력 뉴런 1개(선형 활성화 함수)\n",
    "        - 이진 분류 문제: 출력 뉴런 1개(시그모이드)\n",
    "        - 다중 분류 문제: 출력 뉴런 = 클래스 수(Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c30a95",
   "metadata": {},
   "source": [
    "### 신경망의 동작 원리\n",
    "- 신경망은 입력 데이터를 받아 **순전파**와 **역전파** 과정을 통해 학습\n",
    "\n",
    "1. 순전파\n",
    "    - 데이터를 입력층에서 시작해 은닉층을 거쳐 출력층까지 전달하며 계산을 수행하는 과정\n",
    "    - 각 뉴런은 다음 과정을 거치며 작업\n",
    "        1. 입력값과 가중치를 곱하고 합산\n",
    "        2. 편향 값을 더함\n",
    "        3. 활성화 함수로 변환\n",
    "        4. 결과를 다음 층으로 전달\n",
    "\n",
    "2. 손실 함수\n",
    "    - 출력층에서 예측값과 실제값의 차이를 계산하는 함수\n",
    "    - 대표적인 손실 함수:MSE(회귀 문제), Cross-Entropy Loss(분류 문제)\n",
    "\n",
    "3. 역전파\n",
    "    - 학습 과정에서  손실을 최소화하기 위해 가중치와 편향 값을 조정하는 과정\n",
    "    - 경사하강법 알고리즘을 사용하여 가중치를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95daa47",
   "metadata": {},
   "source": [
    "## 딥러닝의 구조(딥러닝 모델의 기본 구성 요소)\n",
    "### 딥러닝 모델의 기본 구성\n",
    "- 기본적으로 입력층, 은닉층, 출력층으로 구성(기본구조!)\n",
    "    - 이 구성 요소들은 인공 신경망의 구조를 이루는 핵심\n",
    "    - 각각의 층은 데이터를 처리하고 학습을 진행하는 역할을 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c019c7e",
   "metadata": {},
   "source": [
    "- 입력층\n",
    "    - 역할\n",
    "        - 입력층은 **데이터**가 모델로 들어가는 첫 번째 단계\n",
    "        - 모델이 **학습해야 할 데이터를 받아들이는 역할**\n",
    "        - 입력 데이터의 각 특징이 입력층의 뉴런 하나에 연결\n",
    "        - **데이터를 입력받아 아래층으로 넘겨주는 역할!**\n",
    "    \n",
    "    - 특징\n",
    "        - 입력층의 뉴런 수는 데이터의 특징(Feature)개수와 동일\n",
    "        - 입력층은 **계산하지 않고 데이터를 단순히 전달**\n",
    "\n",
    "- 은닉층\n",
    "    - 역할\n",
    "        - 은닉층은 **데이터의 패턴을 학습**하는 역할을 함. 신경망의 연산과 학습 대부분은 은닉층에서 이루어짐\n",
    "        - 입력 데이터를 처리하여 더 높은 수준의 추상적 정보를 추출\n",
    "    \n",
    "    - 특징\n",
    "        - 은닉층의 각 뉴런은 입력 뉴런(또는 이전 층의 뉴런)과 연결되어있으며, 가중치와 편향 값을 사용하여 데이터를 변환\n",
    "        - 각 연결에는 가중치가 부여되고, 뉴런은 가중치와 입력값을 곱한 후 이를 더한 결과를 활성화 함수에 전달\n",
    "\n",
    "    - 활성화 함수\n",
    "        - 활성화 함수는 은닉층의 출력을 비선형(non-linear)으로 변환하여 복잡한 패턴을 학습할 수 있게 함\n",
    "\n",
    "    - 은닉층의 개수와 뉴런 수\n",
    "        - 층 수와 뉴런 수는 모델의 복잡도에 영향을 미침\n",
    "            - 많은 층 → 더 복잡한 패턴 학습 가능 (**하지만 과적합 위험**)\n",
    "            - 뉴런 수가 적으면 학습이 불충분할 수 있음\n",
    "            - **과소적합 주의!!**\n",
    "        - 일반적으로 하이퍼파라미터 튜닝을 통해 최적의 층 수와 뉴런 수를 결정(더 이상 개선이 되지 않는다면, 학습을 중단하는 등의 처리를 한다!)\n",
    "\n",
    "- 출력층\n",
    "    - 출력층은 최종 결과를 제공하는 역할을 하고 은닉층에서 학습된 정보를 바탕으로 예측값을 출력\n",
    "    - 출력층의 뉴런 수는 모델이 해결하려는 문제의 종류에 따라 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fc86c",
   "metadata": {},
   "source": [
    "### 딥러닝 모델의 기본 동작 과정\n",
    "\n",
    "1. 데이터 입력: 입력층에서 데이터가 전달\n",
    "\n",
    "2. 패턴 학습: 은닉층에서 활성화 함수를 사용해 입력 데이터의 패턴을 학습\n",
    "\n",
    "3. 결과 출력: 출력층에서 학습된 정보를 바탕으로 예측값을 계산\n",
    "\n",
    "4. 손실 계산: 예측값과 실제값 간의 차이를 손실 함수를 사용해 계산\n",
    "\n",
    "5. 가중치 업데이트: 역전파와 경사하강법을 사용하여 가중치와 편향을 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b962893",
   "metadata": {},
   "source": [
    "### 딥러닝 모델의 기본 구성\n",
    "- 입력층\n",
    "    - 역할: 데이터를 받아 모델로 전달\n",
    "    - 뉴런 수 결정 기준: 입력 데이터의 특징(feature) 개수\n",
    "\n",
    "- 은닉층\n",
    "    - 역할: 패턴을 학습하고 데이터를 가공\n",
    "    - 뉴런 수 결정 기준: 모델의 복잡도 및 학습 목표에 따라 조정\n",
    "\n",
    "- 출력층\n",
    "    - 역할: 최종 결과를 예측하여 출력\n",
    "    - 뉴런 수 결정 기준: 문제의 유형 (회귀, 이진 분류, 다중 분류)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5edda",
   "metadata": {},
   "source": [
    "## 딥러닝 기본 수학\n",
    "- 활성화 함수(ReLU, Sigmoid, Softmax)\n",
    "- 손실 함수(MSE, Cross-Entropy)\n",
    "- 역전파 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb4630",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "- 활성화 함수는 인공 신경망에서 입력 신호를 처리하고 출력 신호를 결정하는 역할 --> 입력 값을 기반으로 출력 값을 비선형적으로 변환하여 신경망이 복잡한 패턴과 관계를 학습\n",
    "\n",
    "- 활성화 함수의 역할\n",
    "    - 비선형성 추가 : 활성화 함수는 신경망에 비선형성을 도입\n",
    "        - 신경망의 각 층에서 입력과 가중치의 선형 조합만 계산하면 전체 네트워크는 선형 함수가 되어, 복잡한 문제를 해결하지 못함\n",
    "        - 활성화 함수는 비선형 변환을 통해 신경망이 복잡한 패턴과 관계를 학습할 수 있음\n",
    "    \n",
    "    - 출력 범위 제한\n",
    "        - 활성화 함수는 출력 값을 특정 범위로 제한하여 학습 안정성을 높이고, 극단적인 값이 네트워크 전체에 영향을 주는 것을 방지\n",
    "\n",
    "    - 특정 특징 강조 : 활성화 함수는 특정 입력 신호를 강화하거나 억제하여 중요한 특징을 강조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bef23e",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)\n",
    "**대부분의 은닉층에서 기본적으로 사용!**\n",
    "\n",
    "- 수식 : 𝑓(𝑥)=max(0,𝑥) \n",
    "\n",
    "- 특징\n",
    "    - **대부분의 은닉층에서 기본적으로 사용**\n",
    "    - 입력 값이 양수일 때 그대로 출력하고, 음수일 때는 0으로 출력\n",
    "    - 계산이 간단하고, 큰 네트워크에서도 잘 작동\n",
    "\n",
    "- 장점\n",
    "    - 계산이 빠르고 학습 속도가 빠름\n",
    "    - 기울기 소실(Vanishing Gradient) 문제를 줄여 줌\n",
    "\n",
    "- 단점\n",
    "    - 입력 값이 항상 0 이하로 유지되면 뉴런이 비활성화되어 학습하지 못하는 문제가 생김\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d42df4",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "- 특징\n",
    "    - 입력 값을 0과 1 사이로 변환 --> 출력 값이 확률처럼 보이기 때문에 **이진 분류 문제**에서 자주 사용\n",
    "    - 출력층에서 확률 값이 필요한 이진 분류 문제에 사용\n",
    "\n",
    "- 장점\n",
    "    - 출력 값이 제한되어 있어 안정적임\n",
    "\n",
    "- 단점\n",
    "    - 기울기 소실 문제가 발생할 수 있음\n",
    "    - 입력 값이 너무 크거나 작으면 출력의 변화가 거의 없어 학습이 어려워 짐\n",
    "    - 계산이 상대적으로 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f860d05",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "- 특징\n",
    "    - 출력 값을 확률로 변환하여, 모든 출력 값의 합이 1이 되도록 만듦\n",
    "    - **다중 클래스 분류** 문제에서 사용\n",
    "\n",
    "- 장점\n",
    "    - 출력 값이 확률 분포를 가지므로 직관적\n",
    "\n",
    "- 단점\n",
    "    - 계산이 비교적 복잡하며, 클래스 수가 많아지면 느려질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77ffcf",
   "metadata": {},
   "source": [
    "### 손실 함수\n",
    "- 손실 함수는 모델이 예측한 값과 실제 값 간의 차이를 측정하는 함수\n",
    "- 신경망이 학습할 때 손실 값을 최소화하는 방향으로 가중치가 조정 됨 --> 즉, 손실 함수는 학습의 방향과 성능을 평가하는 기준임\n",
    "\n",
    "- 손실함수의 기능\n",
    "    - 성능 평가 : 모델이 얼마나 잘 작동하고 있는지 알려줌\n",
    "    - 학습 방향 제공 : 손실 값을 기준으로 경사 하강법(Gradient Descent)이 실행되어 모델의 가중치를 조정 **(Adam (효율적인 경사 하강법 알고리즘))**\n",
    "    - 문제 유형에 따라 손실 함수 선택 : **회귀와 분류 문제에 적합한 손실 함수가 다름**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e773892",
   "metadata": {},
   "source": [
    "### MSE (Mean Squared Error)\n",
    "**회귀 문제에 사용!**\n",
    "- MSE는 예측 값과 실제 값의 차이를 제곱하여 평균을 구한 값\n",
    "    - 𝑦𝑖 : 실제 값\n",
    "    - 𝑦^𝑖 : 모델의 예측 값\n",
    "    - 𝑛 : 데이터의 개수\n",
    "\n",
    "- 간단하고 계산이 쉽고 예측 오류의 크기에 민감해 큰 오류에 더 큰 패널티를 줌\n",
    "- 오류가 큰 데이터에 민감합니다(이상치의 영향)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae662",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "**분류 모델에 사용!**\n",
    "- Cross-Entropy는 분류 문제에서 모델이 출력한 확률 분포와 실제 레이블의 차이를 측정\n",
    "    - 𝑦𝑖 : 실제 레이블 (0 또는 1)\n",
    "    - 𝑦^𝑖 : 모델이 예측한 확률 값\n",
    "    - 𝑛 : 데이터의 개수\n",
    "\n",
    "- 확률 값(0~1)을 기반으로 계산하므로 분류 문제에 적합\n",
    "- 모델이 잘못된 확률을 출력했을 때 큰 패널티를 줌\n",
    "- 예측 값이 0에 가까울 때( 예 : 𝑦^=0.0001 ) 손실 값이 매우 커질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bcc29",
   "metadata": {},
   "source": [
    "### 역전파\n",
    "\n",
    "- 역전파는 신경망의 학습 과정에서 오류를 기반으로 가중치를 조정하는 알고리즘\n",
    "- 이 알고리즘은 신경망이 입력 데이터를 출력으로 변환하는 과정에서 발생한 오류를 각 층으로 **역방향 전파하여 각 가중치를 조정하는** 데 사용\n",
    "\n",
    "- 역전파 과정의 세부 단계\n",
    "    1. 순방향 전달\n",
    "        - 입력 데이터를 각 층에 전달하여 출력을 계산\n",
    "        - 각 뉴런에서 활성화 함수를 적용하여 비선형성을 추가\n",
    "        - 마지막 층에서 출력 값을 생성\n",
    "\n",
    "    2. 손실 계산\n",
    "        - 손실 함수(Loss Function)를 사용하여 예측 값과 실제 값 간의 손실을 계산\n",
    "        - 예를 들어, MSE를 사용하여 계산\n",
    "\n",
    "    3. 오류 역전파\n",
    "        - 손실 함수의 출력 값을 기준으로, 출력층에서 시작해 입력층으로 이동하며 각 가중치에 대한 기울기를 계산\n",
    "        - 이 과정은 체인 룰(Chain Rule)을 사용하여 이루어짐\n",
    "            - 체인 룰 : 복잡한 함수의 미분을 계산할 때, 각 단계별 미분 값을 곱하는 방법\n",
    "        - 각 뉴런의 출력에 대한 손실의 변화량(기울기)을 계산하고, 이를 다음 층으로 전달\n",
    "\n",
    "\n",
    "    4. 가중치 업데이트\n",
    "        - 경사 하강법(Gradient Descent)을 사용하여 가중치를 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ac1de",
   "metadata": {},
   "source": [
    "## 딥러닝 프레임워크\n",
    "- 딥러닝 프레임워크(Keras, TensorFlow)의 이해\n",
    "- 간단한 신경망 모델 구성 및 훈련\n",
    "\n",
    "- 2진 데이터 --> 이미지, 동영상, 음원 등의 처리\n",
    "- TensorFlow를 조금 더 쉽게 사용할 수 있도록 도와주는 Keras. (같이 설치된다!)\n",
    "    - 무조건 최신 버전의 파이썬을 지원하지 않는다. 현재 3.9버전 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c748b41",
   "metadata": {},
   "source": [
    "### 딥러닝 프레임워크\n",
    "- 딥러닝 모델을 쉽고 효율적으로 개발할 수 있도록 도와주는 도구\n",
    "    - 복잡한 수학적 연산이나 하드웨어 자원의 효율적인 사용을 **프레임워크**가 처리해주기 때문에, 연구자나 개발자는 모델 설계와 데이터 분석에 집중\n",
    "\n",
    "- 가장 널리 사용되는 프레임워크 중 하나가 TensorFlow\n",
    "- Keras는 TensorFlow의 고수준 API로 제공\n",
    "- 언어 모델(LLM등 자연어 처리)에는 pytorch가 좋은 모습\n",
    "\n",
    "- TensorFlow\n",
    "    - 구글에서 개발한 오픈소스 딥러닝 라이브러리로, 다양한 플랫폼에서 실행 가능\n",
    "    - **복잡한 수학적 연산(행렬 연산, 미분 계산 등)을 자동으로 처리**\n",
    "        - 데이터과학자 등의 전문가들은 수학, 통계에 관한 함수들을 커스텀하여 처리하기도 한다.\n",
    "    - 딥러닝뿐만 아니라 머신러닝, 강화 학습에도 활용될 수 있음\n",
    "\n",
    "    - 특징\n",
    "        - 유연성: 저수준의 연산도 제어 가능. 필요에 따라 매우 복잡한 모델 설계 가능\n",
    "        - 멀티플랫폼 지원: CPU, GPU, TPU에서 실행 가능\n",
    "        - 확장성: 대규모 분산 학습을 지원\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b74563",
   "metadata": {},
   "source": [
    "- Keras\n",
    "    - keras는 tensorflow 위에서 작동하는 고수준의 딥러닝 API\n",
    "    - 초보자도 쉽게 사용할 수 있도록 설계되었음\n",
    "    - 복잡한 코드 없이 직관적으로 모델을 설계하고 학습시킬 수 있음\n",
    "    - 다양한 딥러닝 구조(CNN, RNN등)을 지원하며, 필요에 따라 커스터마이징\n",
    "\n",
    "    - 특징\n",
    "        - 간결함: 코드가 간단하고 직관적\n",
    "        - 빠른 프로토타이핑: 모델을 빠르게 설계하고 실험 가능\n",
    "        - 유연성: 다양한 백엔드 엔진 지원\n",
    "\n",
    "\n",
    "- TensorFlow와 Keras의 관계\n",
    "    - TensorFlow는 엔진, Keras는 운전대처럼 생각할 수 있음\n",
    "    - Keras를 사용하면 TensorFlow의 복잡한 저수준 작업을 몰라도 쉽게 모델을 설계하고 훈련\n",
    "    - 최신 TensorFlow(2.0 이상)에서는 Keras가 기본 내장되어 있어 설치와 사용이 더 간단함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa941f",
   "metadata": {},
   "source": [
    "## 딥러닝 모델 실습 - 이진 분류\n",
    "- 이진 분류 문제 해결\n",
    "- 데이터셋 준비 및 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a9d10",
   "metadata": {},
   "source": [
    "### 이진 분류\n",
    "- 이진 분류(Binary Classification)는 주어진 데이터를 두 개의 클래스로 구분하는 문제를 해결하는 과정\n",
    "    - 예: 이메일 스팸 필터링(스팸 또는 정상), 의료 진단(양성 또는 음성), 고객 행동 예측(구매 또는 비구매) 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43320e47",
   "metadata": {},
   "source": [
    "1. 문제 정의\n",
    "    - 주어진 입력 데이터(특성, Features)를 기반으로 해당 데이터가 두 개의 클래스 중 어디에 속하는지 예측\n",
    "    - 출력 --> 클래스 0 또는 클래스 1의 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff827a4",
   "metadata": {},
   "source": [
    "2. 데이터 준비\n",
    "    - 데이터 수집\n",
    "        - **문제에 맞는 데이터셋을 확보**. 예시: 의료 데이터, 고객 행동 데이터, 텍스트 데이터 등\n",
    "    \n",
    "    - 데이터 전처리\n",
    "        - **특성 선택 및 정규화**: 숫자 데이터를 정규화 하거나 범주형 데이터를 인코딩(Label Encoding, One-Hot Encoding). 딥러닝에서는 라벨 인코딩에 더해 원-핫 인코딩까지 해주어야 한다!\n",
    "\n",
    "        - 결측값 처리: 결측값을 제거하거나 보완\n",
    "\n",
    "        - 데이터 분할 \n",
    "            - 데이터를 훈련(Train), 검증(Validation), 테스트(Test) 데이터셋으로 나눔\n",
    "            - 일반적인 비율: 70%(Train) - 15%(Validation) - 15%(Test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65da79",
   "metadata": {},
   "source": [
    "3. 모델 설계\n",
    "- 모델 설계 : 딥러닝 기반 이진 분류 모델은 다음과 같은 구조를 가질 수 있음\n",
    "    - 입력층\n",
    "        - 데이터의 특성 수에 따라 노드 수 결정\n",
    "        - 예: 데이터가 10개의 특성을 가진다면 입력층의 노드 수는 10개\n",
    "\n",
    "    - 은닉층\n",
    "        - Fully Connected Layer 사용.\n",
    "        - 활성화 함수(Activation Function) : ReLU(Rectified Linear Unit)를 주로 사용\n",
    "        - 과적합 방지를 위해 Dropout Layer 추가 가능\n",
    "\n",
    "    - 출력층 노드 수\n",
    "        - 1 (이진 분류는 하나의 확률값 출력)\n",
    "        - 활성화 함수 : **Sigmoid (출력값을 0~1 사이의 확률로 변환)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519e7ad",
   "metadata": {},
   "source": [
    "4. 모델 컴파일\n",
    "- 손실 함수 : Binary Cross-Entropy\n",
    "- 최적화 함수 : Adam (또는 SGD)\n",
    "- 평가지표 : 정확도(Accuracy), F1-Score, Precision, Recall 등."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371dd9bb",
   "metadata": {},
   "source": [
    "5. 모델 훈련\n",
    "- 훈련 데이터를 사용하여 모델을 학습\n",
    "- 동시에 검증 데이터를 사용하여 모델의 성능을 평가하고 과적합을 방지\n",
    "    - 검증 데이터(Validation Set):\n",
    "        - 훈련 중 모델 성능을 측정하는 데 사용\n",
    "        - 테스트 데이터와는 분리되어야 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6fa7ee",
   "metadata": {},
   "source": [
    "6. 과적합 방지\n",
    "- **call back 함수**를 이용하여 훈련을 중단한다! --> **과적합 방지 목적!!**\n",
    "\n",
    "- Early Stopping\n",
    "    - 훈련 중 검증 성능이 더 이상 향상되지 않으면 조기에 학습을 멈춤\n",
    "    - 과적합을 방지하는 데 효과적\n",
    "\n",
    "- Dropout\n",
    "    - 학습 중 일부 노드를 무작위로 제외하여 과적합을 방지\n",
    "    - 뉴런의 수가 8개라면 Dropout(0.5)이후 4개만 활성화!\n",
    "    - dropout도 하나의 층(Layer)이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1355e",
   "metadata": {},
   "source": [
    "7. 모델 평가\n",
    "- 훈련 후 테스트 데이터를 사용해 모델 성능을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966a8ef",
   "metadata": {},
   "source": [
    "8. 예측\n",
    "- 새로운 데이터를 입력하여 예측값을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3612c6",
   "metadata": {},
   "source": [
    "### 데이터셋\n",
    "- 훈련(Training), 검증(Validation), 테스트(Test) 데이터셋 --> 머신러닝과 딥러닝 모델 학습 과정에서 중요한 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f637c32",
   "metadata": {},
   "source": [
    "1. 훈련 데이터셋 (Training Dataset)\n",
    "    - 정의모델을 학습시키는 데 사용되는 데이터셋\n",
    "        - 입력 데이터(X)와 그에 해당하는 실제 레이블(정답, y)을 포함\n",
    "    \n",
    "    - 역할\n",
    "        - 모델이 데이터의 패턴과 관계를 학습하도록 하고 모델의 가중치와 바이어스를 조정하는 데 사용\n",
    "        - 학습 과정에서 손실(Loss)을 계산하고 이를 기반으로 최적화 알고리즘이 모델을 업데이트\n",
    "        - 데이터의 대부분(약 70~80%)을 차지하며, 모델이 충분히 학습할 수 있도록 많은 데이터를 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1b691",
   "metadata": {},
   "source": [
    "2. 검증 데이터셋 (Validation Dataset)\n",
    "    - 훈련 중 모델의 성능을 평가하기 위해 사용되는 데이터셋\n",
    "    - 훈련 데이터와 테스트 데이터와는 별도로 분리된 데이터\n",
    "\n",
    "    - 역할\n",
    "        - 모델이 훈련 데이터에만 치우치지 않고 새로운 데이터에도 잘 작동하는지 평가 --> 과적합(Overfitting)을 감지하는 데 사용\n",
    "        - 하이퍼파라미터(예: 학습률, 은닉층 수, 드롭아웃 비율)를 튜닝할 때 활용\n",
    "    \n",
    "    - 특징\n",
    "        - 훈련 중에는 검증 데이터를 사용하여 모델의 성능 변화를 실시간으로 모니터링\n",
    "        - Early Stopping과 같은 기법에서 검증 데이터의 손실이 일정 수준 이상 줄어들지 않으면 학습 중단\n",
    "        - 일반적으로 전체 데이터의 약 10~15%를 차지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383532e",
   "metadata": {},
   "source": [
    "3. 테스트 데이터셋\n",
    "    - 학습이 완료된 후, 모델의 최종 성능을 평가하기 위해 사용되는 데이터셋\n",
    "    - 모델이 훈련이나 검증에서 본 적 없는 새로운 데이터로 구성\n",
    "\n",
    "    - 역할\n",
    "        - 모델이 실제 환경에서 얼마나 잘 동작할지를 예측\n",
    "        - 훈련 및 검증 데이터를 사용하지 않았으므로 모델의 일반화 능력을 평가하는 데 적합\n",
    "        - 최종 평가 결과를 통해 모델의 신뢰성과 성능을 판단\n",
    "\n",
    "    - 특징\n",
    "        - 테스트 데이터는 모델이 학습 과정에서 한 번도 접하지 않았기 때문에, \"새로운 데이터\"에 대한 모델의 성능을 측정 --> 과적합이 심한 모델은 테스트 데이터에서 성능이 낮아질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b836b31",
   "metadata": {},
   "source": [
    "4. 데이터 분할 비율\n",
    "    - 보통 다음과 같은 비율로 데이터를 나눔\n",
    "        - 훈련 데이터: 70~80%\n",
    "        - 검증 데이터: 10~15%\n",
    "        - 테스트 데이터: 10~15%\n",
    "    \n",
    "    - 데이터가 적은 경우 교차 검증(Cross-Validation)을 활용해 데이터를 효율적으로 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57e1e2",
   "metadata": {},
   "source": [
    "### 데이터셋을 나누는 이유는?\n",
    "\n",
    "1. 과적합(Overfitting) 방지\n",
    "    - 훈련 데이터만 사용하면 모델이 해당 데이터에만 최적화 --> 새로운 데이터에 대한 예측 성능이 떨어질 가능성이 높음\n",
    "    - 검증 데이터와 테스트 데이터를 통해 과적합 여부를 확인\n",
    "\n",
    "2. 모델 성능 평가\n",
    "    - 테스트 데이터는 실제 환경에서 모델이 얼마나 잘 작동할지를 평가하기 위한 기준이 됨\n",
    "\n",
    "3.  데이터 유출 방지\n",
    "    - 검증 또는 테스트 데이터가 훈련 데이터에 섞이면 --> 모델은 실제로 학습하지 않은 데이터를 일반화하는 것이 아니라, 그 데이터를 \"암기\"할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fbad1",
   "metadata": {},
   "source": [
    "## 딥러닝 모델 실습 - 다중 분류\n",
    "- 다중 분류 문제 해결\n",
    "- Softmax 활성화 함수와 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d13f2",
   "metadata": {},
   "source": [
    "### 다중 분류 문제\n",
    "- 입력 데이터를 여러 클래스 중 하나로 분류하는 문제\n",
    "    - 예시: \n",
    "        - 고양이, 개, 새 등 여러 동물 이미지를 특정 클래스로 분류\n",
    "        - 여러 종류의 꽃을 분류하는 작업\n",
    "        - 손글씨 숫자 데이터(MNIST)에서 숫자 0부터 9까지를 분류하는 문제\n",
    "        - 영화 리뷰를 긍정, 중립, 부정으로 분류하는 문제\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44730ff9",
   "metadata": {},
   "source": [
    "### 다중 분류 문제를 해결하기 위한 과정\n",
    "1. 데이터 준비\n",
    "    1. 데이터 수집: 사용할 데이터셋을 준비(데이터는 각 클래스에 해당하는 레이블이 있어야 함)\n",
    "\n",
    "    2. 데이터 전처리\n",
    "        - One-Hot Encoding: 레이블을 숫자로 인코딩하고, 이를 원-핫 **벡터**로 변환\n",
    "\n",
    "        - 정규화: 입력 데이터를 정규화 하여 신경망 학습을 안정적으로 처리 --> 이미지의 픽셀 값(0~255)을 [0, 1] 범위로 스케일링.\n",
    "\n",
    "    3. 데이터 분리\n",
    "        - 데이터를 훈련, 검증, 테스트 세트로 나눔 → 일반적으로 70%(훈련), 15%(검증), 15%(테스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50848998",
   "metadata": {},
   "source": [
    "2. 모델 설계\n",
    "- 입력층\n",
    "    - 데이터의 차원과 동일한 입력을 받는 층\n",
    "\n",
    "- 은닉층\n",
    "    - Fully Connected Layer(완전 연결층) 또는 Convolutoinal Layer(이미지 문제의 경우)를 포함\n",
    "    - 활성화 함수로  ReLU를 사용하여 비선형성을 추가\n",
    "\n",
    "- 출력층\n",
    "    - 출력 뉴런의 수는 클래스의 수와 동일. 예를 들어, 클래스가 3개라면 출력층의 뉴런 수는 3\n",
    "    - **Softmax 활성화함수를 사용하여** 각 클래스에 대한 확률 계산 --> Softmax는 각 뉴런의 출력을 확률로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc40170",
   "metadata": {},
   "source": [
    "3. 모델 훈련\n",
    "    1. 손실 함수\n",
    "        - 다중 분류 문제에서는 Categorical Crossentropy 손실 함수를 사용\n",
    "\n",
    "    2. 옵티마이저\n",
    "        - Gradient Descent 기반 알고리즘(예: Adam)을 사용하여 가중치를 업데이트\n",
    "    \n",
    "    3. 훈련 프로세스\n",
    "        - 모델에 훈련 데이터를 입력하고, 반복적으로 가중치를 업데이트하여 손실을 최소화\n",
    "        - 검증 데이터를 사용하여 과적합(overfitting)을 방지하고 성능을 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273040b7",
   "metadata": {},
   "source": [
    "4. 모델 평가\n",
    "    - 정확도: 테스트 데이터에서 모델이 얼마나 정확하게 예측하는지 평가\n",
    "\n",
    "    - 혼동 행렬: 각 클래스에 대한 잘못된 예측과 정확한 예측을 시각화\n",
    "\n",
    "    - 클래스별 평가: 클래스별 정밀도, 재현율, F1 점수를 계산하여 모델의 성능을 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd7bd6",
   "metadata": {},
   "source": [
    "5. 예측\n",
    "    - 학습된 모델을 사용하여 새로운 데이터를 예측\n",
    "    - **예측은 Softmax 출력을 기반으로 가장 높은 확률 값을 가진 클래스를 선택**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d524662",
   "metadata": {},
   "source": [
    "### MNIST 데이터셋을 활용한 이미지 분류\n",
    "- tensor_proj의 tensor_1 파일에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986ce6a",
   "metadata": {},
   "source": [
    "### 전체 딥러닝 프로세스 적용(코드 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51845d29",
   "metadata": {},
   "source": [
    "1. 데이터 로드 및 전처리\n",
    "    - x_train과 x_test는 입력 데이터(이미지)\n",
    "    - 데이터를 [0, 1] 범위로 정규화 하여 신경망을 학습\n",
    "    - 레이블(y_train, y_test)은 원-핫 인코딩하여 다중 분류 작업에 맞게 변환\n",
    "\n",
    "2. 모델 구성\n",
    "    - 입력층 → 은닉층 → 출력층으로 이루어진 신경망을 설계\n",
    "    - 출력층의 Softmax 활성화 함수는 각 클래스에 대한 확률을 계산\n",
    "\n",
    "3. 모델 컴파일\n",
    "    - 손실 함수로 Categorical Crossentropy를 사용하여 실제 값과 예측 확률의 차이를 줄이는 방향으로 학습\n",
    "    - 옵티마이저로 Adam을 사용하여 모델의 가중치를 효율적으로 업데이트\n",
    "\n",
    "4. 모델 훈련\n",
    "    - fit 메서드를 사용해 모델을 훈련\n",
    "    - 훈련 데이터를 기반으로 가중치를 학습하고, 검증 데이터를 통해 과적합을 방지\n",
    "\n",
    "5. 모델 평가\n",
    "    - evaluate 메서드를 사용해 테스트 데이터로 모델의 성능(정확도)을 평가\n",
    "\n",
    "6. 예측\n",
    "    - • 훈련된 모델을 사용해 새로운 데이터의 클래스를 예측\n",
    "    - Softmax 출력은 각 클래스에 대한 확률 값을 반환하고, argmax를 사용해 가장 높은 확률의 클래스를 선택"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
